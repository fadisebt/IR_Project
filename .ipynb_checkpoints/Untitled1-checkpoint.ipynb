{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa1eb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "def word(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    return words\n",
    "\n",
    "def tage(words):\n",
    "    tags = nltk.pos_tag(words) \n",
    "    return tags\n",
    "\n",
    "def wordTags(words,tags):\n",
    "    notTags=[\"SYM\",\"PRP$\",\"CC\",\"PRP\",\"TO\",\"IN\",\"DT\",\"EX\",\"FW\",\"MD\",\"PDT\",\"POS\",\"RP\",\"UH\",\"WDT\",\"WP\",\"WP$\",\"WRB\",\"$\",\"(\",\")\",\",\",\"--\",\".\",\":\"]\n",
    "    adv=[\"JJR\",\"JJS\"]\n",
    "    verb=[\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "    adverb=[\"RB\",\"RBR\",\"RBS\"]\n",
    "    nouns=[\"NN\",\"NNP\",\"NNPS\",\"NNS\"]\n",
    "    exact=False\n",
    "    last=[]\n",
    "    for word, tag in tags:\n",
    "        if tag==\"``\":\n",
    "            exact=!exact \n",
    "            last.append(word)\n",
    "        elif exact:\n",
    "            last.append(word)\n",
    "        elif tag in adv:\n",
    "            last.append(lemmatizer.lemmatize(word, pos=\"a\"))\n",
    "        elif tag in verb:\n",
    "            last.append(lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        elif tag in adverb:\n",
    "            last.append(lemmatizer.lemmatize(word, pos=\"r\"))\n",
    "        elif tag in nouns:\n",
    "            last.append(lemmatizer.lemmatize(word, pos=\"n\"))\n",
    "        elif tag not in notTags:\n",
    "            last.append(lemmatizer.lemmatize(word, pos=\"s\"))\n",
    "    return (last)\n",
    "\n",
    "def without_prepositions(last):\n",
    "    sentence_without_prepositions = \"\".join(last)\n",
    "    return (sentence_without_prepositions) \n",
    "\n",
    "def inverted_index (docs):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        inverted_index[token].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "def index_hash(word,data):\n",
    "    index = {}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b50b4367",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3260187269.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[131], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    f = open(\"C:\\Users\\hp\\Desktop\\antique-collection.txt\", \"rb\")\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# تعريف المستند الجديد\n",
    "new_doc = \"Hello world again\"\n",
    "\n",
    "# الحصول على رقم المستند الجديد بزيادة عدد المستندات الحالية بواحد\n",
    "new_doc_id = len(docs)\n",
    "\n",
    "# إضافة المستند الجديد إلى قائمة المستندات\n",
    "docs.append(new_doc)\n",
    "\n",
    "# تحليل وتجزئة النص إلى كلمات باستخدام nltk\n",
    "tokens = nltk.word_tokenize(new_doc)\n",
    "\n",
    "# تكرار على كل كلمة\n",
    "for token in tokens:\n",
    "  # إضافة رقم المستند الجديد إلى قائمة الكلمة في القاموس\n",
    "  inverted_index[token].append(new_doc_id)\n",
    "\n",
    "# طباعة قائمة التداخل بعد الإضافة\n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a1739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'Hello': [0, 1], 'world': [0], 'Python': [1, 2], 'is': [2], 'awesome': [2]})\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "# تعريف المستندات النصية\n",
    "docs = [\"Hello world\", \"Hello Python\", \"Python is awesome\"]\n",
    "\n",
    "# إنشاء قاموس فارغ لحفظ قائمة التداخل\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "# تكرار على كل مستند\n",
    "for doc_id, doc in enumerate(docs):\n",
    "  # تحليل وتجزئة النص إلى كلمات باستخدام nltk\n",
    "  tokens = nltk.word_tokenize(doc)\n",
    "  # تكرار على كل كلمة\n",
    "  for token in tokens:\n",
    "    # إضافة رقم المستند إلى قائمة الكلمة في القاموس\n",
    "    inverted_index[token].append(doc_id)\n",
    "\n",
    "# طباعة قائمة التداخل\n",
    "print(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f4ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'تصميم': ['https://example.com/article1'], 'واجهة المستخدم': ['https://example.com/article1'], 'تجربة المستخدم': ['https://example.com/article1'], 'برمجة': ['https://example.com/article2'], 'بايثون': ['https://example.com/article2'], 'تعلم الآلة': ['https://example.com/article2'], 'تسويق': ['https://example.com/article3'], 'سيو': ['https://example.com/article3'], 'محتوى': ['https://example.com/article3']}\n"
     ]
    }
   ],
   "source": [
    "# تعريف مجموعة البيانات، وهي قائمة من المقالات\n",
    "articles = [\n",
    "  {\"url\": \"https://example.com/article1\", \"keywords\": [\"تصميم\", \"واجهة المستخدم\", \"تجربة المستخدم\"]},\n",
    "  {\"url\": \"https://example.com/article2\", \"keywords\": [\"برمجة\", \"بايثون\", \"تعلم الآلة\"]},\n",
    "  {\"url\": \"https://example.com/article3\", \"keywords\": [\"تسويق\", \"سيو\", \"محتوى\"]}\n",
    "]\n",
    "\n",
    "# إنشاء جدول هاش فارغ لتخزين الفهرس\n",
    "index = {}\n",
    "\n",
    "# تكرار على كل مقال في مجموعة البيانات\n",
    "for article in articles:\n",
    "  # تكرار على كل كلمة رئيسية في المقال\n",
    "  for keyword in article[\"keywords\"]:\n",
    "    # إذا كانت الكلمة الرئيسية غير موجودة في جدول هاش، فإضافة قائمة جديدة تحتوي على رابط المقال\n",
    "    if keyword not in index:\n",
    "      index[keyword] = [article[\"url\"]]\n",
    "    # إذا كانت الكلمة الرئيسية موجودة في جدول هاش، فإضافة رابط المقال إلى القائمة الموجودة\n",
    "    else:\n",
    "      index[keyword].append(article[\"url\"])\n",
    "\n",
    "# طباعة جدول هاش للفهرس\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825aefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a210079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
